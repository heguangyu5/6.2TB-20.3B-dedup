原帖1: [要对单个 6.20TB 的超大 csv 文件保持顺序的情况下进行去除重复行，有什么好思路？显然不可能加载进内存](https://www.v2ex.com/t/1046023)

原帖2: [单个 6.20TB 的超大 csv 文件保持顺序的情况下进行去除重各个方案的可行性分析](https://www.v2ex.com/t/1046610)



这里给出两个解决方案,这两个解决方案的基本思路都是基于hash的去重.

1. `dedup-use-more-mem` 

    用时少,内存需求略多.

    每1亿行数据需内存1.34GB,实测处理20亿行用时36分钟.

    那么203亿行需内存约272GB,用时约6个小时.

2. `dedup-use-less-mem` 

    内存需求是1的一半,用时大约比1多1倍

    每1亿行数据需内存0.67GB,实测处理20亿行用时57分钟.

    那么203亿行需内存约136GB,用时约10个小时.



实测服务器配置:

Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz

32GB内存

西数机械硬盘 4TB * 4, 做了Raid 0

Ubuntu 22.04 server amd64



`gen-20.3B-lines.sh` 可生成6.6TB >203亿行的数据,生成数据耗时约7小时.